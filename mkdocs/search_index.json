{
    "docs": [
        {
            "location": "/", 
            "text": "DiffBase.jl\n\n\nDiffBase is a Julia package that provides common utilities and test functions that are used by various JuliaDiff packages.\n\n\nIf you're a user of any JuliaDiff packages, you may want to check out DiffBase's \nDiffResult API\n, which can be used in conjunction with other packages in order to compute multiple orders of derivatives simultaneously.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#diffbasejl", 
            "text": "DiffBase is a Julia package that provides common utilities and test functions that are used by various JuliaDiff packages.  If you're a user of any JuliaDiff packages, you may want to check out DiffBase's  DiffResult API , which can be used in conjunction with other packages in order to compute multiple orders of derivatives simultaneously.", 
            "title": "DiffBase.jl"
        }, 
        {
            "location": "/diffresult/", 
            "text": "DiffResult API Documentation\n\n\nSupported By:\n\n\n\n\nForwardDiff\n\n\nReverseDiff\n\n\n\n\nMany differentiation techniques can calculate primal values and multiple orders of derivatives simultaneously. In other words, there are techniques that allow one to compute \nf(x)\n, \n\u2207f(x)\n and \nH(f(x))\n in one fell swoop!\n\n\nFor this purpose, DiffBase provides the \nDiffResult\n type, which can be passed to in-place differentiation methods instead of an output buffer. All results computed by the method are loaded into the given \nDiffResult\n, which the user can then query using DiffBase's API.\n\n\nHere's an example of the \nDiffResult\n in action using ForwardDiff:\n\n\njulia\n using ForwardDiff, DiffBase\n\njulia\n f(x) = sum(sin, x) + prod(tan, x) * sum(sqrt, x);\n\njulia\n x = rand(4);\n\n# construct a `DiffResult` with storage for a Hessian, gradient,\n# and primal value based on the type and shape of `x`.\njulia\n result = DiffBase.HessianResult(x)\n\n# instead of passing an output buffer to `hessian!`, we pass `result`\njulia\n ForwardDiff.hessian!(result, f, x);\n\n# ...and now we can get all the computed data from `result`\njulia\n DiffBase.value(result) == f(x)\ntrue\n\njulia\n DiffBase.gradient(result) == ForwardDiff.gradient(f, x)\ntrue\n\njulia\n DiffBase.hessian(result) == ForwardDiff.hessian(f, x)\ntrue\n\n\n\n\nThe rest of this document describes the API for constructing, accessing, and mutating \nDiffResult\n instances. For details on how to use a \nDiffResult\n with a specific package's methods, please consult that package's documentation.\n\n\n\n\nConstructing DiffResults\n\n\n#\n\n\nDiffBase.DiffResult\n \n \nType\n.\n\n\nDiffResult(value, derivs::Tuple)\n\n\n\n\nReturn a \nDiffResult\n instance where values will be stored in the provided \nvalue\n storage and derivatives will be stored in the provided \nderivs\n storage.\n\n\nNote that the arguments can be \nNumber\ns or \nAbstractArray\ns, depending on the dimensionality of your target function.\n\n\nsource\n\n\nDiffResult(value, derivs...)\n\n\n\n\nEquivalent to \nDiffResult(value, derivs::Tuple)\n, where \nderivs...\n is the splatted form of \nderivs::Tuple\n.\n\n\nsource\n\n\n#\n\n\nDiffBase.JacobianResult\n \n \nFunction\n.\n\n\nJacobianResult(x::AbstractArray)\n\n\n\n\nConstruct a \nDiffResult\n that can be used for Jacobian calculations where \nx\n is the input to the target function. This method assumes that the target function's output dimension equals its input dimension.\n\n\nNote that \nJacobianResult\n allocates its own storage; \nx\n is only used for type and shape information. If you want to allocate storage yourself, use the \nDiffResult\n constructor instead.\n\n\nsource\n\n\nJacobianResult(y::AbstractArray, x::AbstractArray)\n\n\n\n\nConstruct a \nDiffResult\n that can be used for Jacobian calculations where \nx\n is the input to the target function, and \ny\n is the output (e.g. when taking the Jacobian of \nf!(y, x)\n).\n\n\nLike the single argument version, \ny\n and \nx\n are only used for type and shape information and are not stored in the returned \nDiffResult\n.\n\n\nsource\n\n\n#\n\n\nDiffBase.GradientResult\n \n \nFunction\n.\n\n\nGradientResult(x::AbstractArray)\n\n\n\n\nConstruct a \nDiffResult\n that can be used for gradient calculations where \nx\n is the input to the target function.\n\n\nNote that \nGradientResult\n allocates its own storage; \nx\n is only used for type and shape information. If you want to allocate storage yourself, use the \nDiffResult\n constructor instead.\n\n\nsource\n\n\n#\n\n\nDiffBase.HessianResult\n \n \nFunction\n.\n\n\nHessianResult(x::AbstractArray)\n\n\n\n\nConstruct a \nDiffResult\n that can be used for Hessian calculations where \nx\n is the input to the target function.\n\n\nNote that \nHessianResult\n allocates its own storage; \nx\n is only used for type and shape information. If you want to allocate storage yourself, use the \nDiffResult\n constructor instead.\n\n\nsource\n\n\n\n\nAccessing data from DiffResults\n\n\n#\n\n\nDiffBase.value\n \n \nFunction\n.\n\n\nvalue(r::DiffResult)\n\n\n\n\nReturn the primal value stored in \nr\n.\n\n\nNote that this method returns a reference, not a copy. Thus, if \nvalue(r)\n is mutable, mutating \nvalue(r)\n will mutate \nr\n.\n\n\nsource\n\n\n#\n\n\nDiffBase.derivative\n \n \nFunction\n.\n\n\nderivative(r::DiffResult, ::Type{Val{i}} = Val{1})\n\n\n\n\nReturn the \nith\n derivative stored in \nr\n, defaulting to the first derivative.\n\n\nNote that this method returns a reference, not a copy. Thus, if \nderivative(r)\n is mutable, mutating \nderivative(r)\n will mutate \nr\n.\n\n\nsource\n\n\n#\n\n\nDiffBase.gradient\n \n \nFunction\n.\n\n\ngradient(r::DiffResult)\n\n\n\n\nReturn the gradient stored in \nr\n (equivalent to \nderivative(r)\n).\n\n\nNote that this method returns a reference, not a copy. Thus, if \ngradient(r)\n is mutable, mutating \ngradient(r)\n will mutate \nr\n.\n\n\nsource\n\n\n#\n\n\nDiffBase.jacobian\n \n \nFunction\n.\n\n\njacobian(r::DiffResult)\n\n\n\n\nReturn the Jacobian stored in \nr\n (equivalent to \nderivative(r)\n).\n\n\nNote that this method returns a reference, not a copy. Thus, if \njacobian(r)\n is mutable, mutating \njacobian(r)\n will mutate \nr\n.\n\n\nsource\n\n\n#\n\n\nDiffBase.hessian\n \n \nFunction\n.\n\n\nhessian(r::DiffResult)\n\n\n\n\nReturn the Hessian stored in \nr\n (equivalent to \nderivative(r, Val{2})\n).\n\n\nNote that this method returns a reference, not a copy. Thus, if \nhessian(r)\n is mutable, mutating \nhessian(r)\n will mutate \nr\n.\n\n\nsource\n\n\n\n\nMutating DiffResults\n\n\n#\n\n\nDiffBase.value!\n \n \nFunction\n.\n\n\nvalue!(r::DiffResult, x)\n\n\n\n\nCopy \nx\n into \nr\n's value storage, such that \nvalue(r) == x\n.\n\n\nsource\n\n\nvalue!(f, r::DiffResult, x)\n\n\n\n\nLike \nvalue!(r::DiffResult, x)\n, but with \nf\n applied to each element, such that \nvalue(r) == map(f, x)\n.\n\n\nsource\n\n\n#\n\n\nDiffBase.derivative!\n \n \nFunction\n.\n\n\nderivative!(r::DiffResult, x, ::Type{Val{i}} = Val{1})\n\n\n\n\nCopy \nx\n into \nr\n's \nith\n derivative storage, such that \nderivative(r, Val{i}) == x\n.\n\n\nsource\n\n\nderivative!(f, r::DiffResult, x, ::Type{Val{i}} = Val{1})\n\n\n\n\nLike \nderivative!(r::DiffResult, x, Val{i})\n, but with \nf\n applied to each element, such that \nderivative(r, Val{i}) == map(f, x)\n.\n\n\nsource\n\n\n#\n\n\nDiffBase.gradient!\n \n \nFunction\n.\n\n\ngradient!(r::DiffResult, x)\n\n\n\n\nCopy \nx\n into \nr\n's gradient storage, such that \ngradient(r) == x\n.\n\n\nsource\n\n\ngradient!(f, r::DiffResult, x)\n\n\n\n\nLike \ngradient!(r::DiffResult, x)\n, but with \nf\n applied to each element, such that \ngradient(r) == map(f, x)\n.\n\n\nsource\n\n\n#\n\n\nDiffBase.jacobian!\n \n \nFunction\n.\n\n\njacobian!(r::DiffResult, x)\n\n\n\n\nCopy \nx\n into \nr\n's Jacobian storage, such that \njacobian(r) == x\n.\n\n\nsource\n\n\njacobian!(f, r::DiffResult, x)\n\n\n\n\nLike \njacobian!(r::DiffResult, x)\n, but with \nf\n applied to each element, such that \njacobian(r) == map(f, x)\n.\n\n\nsource\n\n\n#\n\n\nDiffBase.hessian!\n \n \nFunction\n.\n\n\nhessian!(r::DiffResult, x)\n\n\n\n\nCopy \nx\n into \nr\n's Hessian storage, such that \nhessian(r) == x\n.\n\n\nsource\n\n\nhessian!(f, r::DiffResult, x)\n\n\n\n\nLike \nhessian!(r::DiffResult, x)\n, but with \nf\n applied to each element, such that \nhessian(r) == map(f, x)\n.\n\n\nsource", 
            "title": "DiffResult API"
        }, 
        {
            "location": "/diffresult/#diffresult-api-documentation", 
            "text": "Supported By:   ForwardDiff  ReverseDiff   Many differentiation techniques can calculate primal values and multiple orders of derivatives simultaneously. In other words, there are techniques that allow one to compute  f(x) ,  \u2207f(x)  and  H(f(x))  in one fell swoop!  For this purpose, DiffBase provides the  DiffResult  type, which can be passed to in-place differentiation methods instead of an output buffer. All results computed by the method are loaded into the given  DiffResult , which the user can then query using DiffBase's API.  Here's an example of the  DiffResult  in action using ForwardDiff:  julia  using ForwardDiff, DiffBase\n\njulia  f(x) = sum(sin, x) + prod(tan, x) * sum(sqrt, x);\n\njulia  x = rand(4);\n\n# construct a `DiffResult` with storage for a Hessian, gradient,\n# and primal value based on the type and shape of `x`.\njulia  result = DiffBase.HessianResult(x)\n\n# instead of passing an output buffer to `hessian!`, we pass `result`\njulia  ForwardDiff.hessian!(result, f, x);\n\n# ...and now we can get all the computed data from `result`\njulia  DiffBase.value(result) == f(x)\ntrue\n\njulia  DiffBase.gradient(result) == ForwardDiff.gradient(f, x)\ntrue\n\njulia  DiffBase.hessian(result) == ForwardDiff.hessian(f, x)\ntrue  The rest of this document describes the API for constructing, accessing, and mutating  DiffResult  instances. For details on how to use a  DiffResult  with a specific package's methods, please consult that package's documentation.", 
            "title": "DiffResult API Documentation"
        }, 
        {
            "location": "/diffresult/#constructing-diffresults", 
            "text": "#  DiffBase.DiffResult     Type .  DiffResult(value, derivs::Tuple)  Return a  DiffResult  instance where values will be stored in the provided  value  storage and derivatives will be stored in the provided  derivs  storage.  Note that the arguments can be  Number s or  AbstractArray s, depending on the dimensionality of your target function.  source  DiffResult(value, derivs...)  Equivalent to  DiffResult(value, derivs::Tuple) , where  derivs...  is the splatted form of  derivs::Tuple .  source  #  DiffBase.JacobianResult     Function .  JacobianResult(x::AbstractArray)  Construct a  DiffResult  that can be used for Jacobian calculations where  x  is the input to the target function. This method assumes that the target function's output dimension equals its input dimension.  Note that  JacobianResult  allocates its own storage;  x  is only used for type and shape information. If you want to allocate storage yourself, use the  DiffResult  constructor instead.  source  JacobianResult(y::AbstractArray, x::AbstractArray)  Construct a  DiffResult  that can be used for Jacobian calculations where  x  is the input to the target function, and  y  is the output (e.g. when taking the Jacobian of  f!(y, x) ).  Like the single argument version,  y  and  x  are only used for type and shape information and are not stored in the returned  DiffResult .  source  #  DiffBase.GradientResult     Function .  GradientResult(x::AbstractArray)  Construct a  DiffResult  that can be used for gradient calculations where  x  is the input to the target function.  Note that  GradientResult  allocates its own storage;  x  is only used for type and shape information. If you want to allocate storage yourself, use the  DiffResult  constructor instead.  source  #  DiffBase.HessianResult     Function .  HessianResult(x::AbstractArray)  Construct a  DiffResult  that can be used for Hessian calculations where  x  is the input to the target function.  Note that  HessianResult  allocates its own storage;  x  is only used for type and shape information. If you want to allocate storage yourself, use the  DiffResult  constructor instead.  source", 
            "title": "Constructing DiffResults"
        }, 
        {
            "location": "/diffresult/#accessing-data-from-diffresults", 
            "text": "#  DiffBase.value     Function .  value(r::DiffResult)  Return the primal value stored in  r .  Note that this method returns a reference, not a copy. Thus, if  value(r)  is mutable, mutating  value(r)  will mutate  r .  source  #  DiffBase.derivative     Function .  derivative(r::DiffResult, ::Type{Val{i}} = Val{1})  Return the  ith  derivative stored in  r , defaulting to the first derivative.  Note that this method returns a reference, not a copy. Thus, if  derivative(r)  is mutable, mutating  derivative(r)  will mutate  r .  source  #  DiffBase.gradient     Function .  gradient(r::DiffResult)  Return the gradient stored in  r  (equivalent to  derivative(r) ).  Note that this method returns a reference, not a copy. Thus, if  gradient(r)  is mutable, mutating  gradient(r)  will mutate  r .  source  #  DiffBase.jacobian     Function .  jacobian(r::DiffResult)  Return the Jacobian stored in  r  (equivalent to  derivative(r) ).  Note that this method returns a reference, not a copy. Thus, if  jacobian(r)  is mutable, mutating  jacobian(r)  will mutate  r .  source  #  DiffBase.hessian     Function .  hessian(r::DiffResult)  Return the Hessian stored in  r  (equivalent to  derivative(r, Val{2}) ).  Note that this method returns a reference, not a copy. Thus, if  hessian(r)  is mutable, mutating  hessian(r)  will mutate  r .  source", 
            "title": "Accessing data from DiffResults"
        }, 
        {
            "location": "/diffresult/#mutating-diffresults", 
            "text": "#  DiffBase.value!     Function .  value!(r::DiffResult, x)  Copy  x  into  r 's value storage, such that  value(r) == x .  source  value!(f, r::DiffResult, x)  Like  value!(r::DiffResult, x) , but with  f  applied to each element, such that  value(r) == map(f, x) .  source  #  DiffBase.derivative!     Function .  derivative!(r::DiffResult, x, ::Type{Val{i}} = Val{1})  Copy  x  into  r 's  ith  derivative storage, such that  derivative(r, Val{i}) == x .  source  derivative!(f, r::DiffResult, x, ::Type{Val{i}} = Val{1})  Like  derivative!(r::DiffResult, x, Val{i}) , but with  f  applied to each element, such that  derivative(r, Val{i}) == map(f, x) .  source  #  DiffBase.gradient!     Function .  gradient!(r::DiffResult, x)  Copy  x  into  r 's gradient storage, such that  gradient(r) == x .  source  gradient!(f, r::DiffResult, x)  Like  gradient!(r::DiffResult, x) , but with  f  applied to each element, such that  gradient(r) == map(f, x) .  source  #  DiffBase.jacobian!     Function .  jacobian!(r::DiffResult, x)  Copy  x  into  r 's Jacobian storage, such that  jacobian(r) == x .  source  jacobian!(f, r::DiffResult, x)  Like  jacobian!(r::DiffResult, x) , but with  f  applied to each element, such that  jacobian(r) == map(f, x) .  source  #  DiffBase.hessian!     Function .  hessian!(r::DiffResult, x)  Copy  x  into  r 's Hessian storage, such that  hessian(r) == x .  source  hessian!(f, r::DiffResult, x)  Like  hessian!(r::DiffResult, x) , but with  f  applied to each element, such that  hessian(r) == map(f, x) .  source", 
            "title": "Mutating DiffResults"
        }
    ]
}